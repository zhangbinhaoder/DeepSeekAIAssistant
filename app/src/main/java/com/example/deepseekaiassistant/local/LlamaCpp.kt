package com.example.deepseekaiassistant.local

import android.util.Log
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.atomic.AtomicReference

/**
 * LlamaCpp - llama.cpp çš„ JNI ç»‘å®šç±»
 * 
 * æä¾›ä¸ native å±‚çš„ç¨³å®šæ¥å£ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œç”Ÿæˆå›å¤
 * 
 * æ¶æ„è®¾è®¡ï¼š
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚  LocalAIManager â”‚ â”€â”€â–¶ â”‚     LlamaCpp     â”‚ â”€â”€â–¶ â”‚ llama_android.cppâ”‚
 * â”‚   (çŠ¶æ€ç®¡ç†)     â”‚     â”‚   (JNI ç»‘å®šå±‚)    â”‚     â”‚   (Native å®ç°)  â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 * 
 * æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
 * 1. Native æ¨¡å¼ï¼šçœŸæ­£çš„ llama.cpp æ¨ç†ï¼ˆéœ€è¦ç¼–è¯‘ native åº“ + åŠ è½½æ¨¡å‹ï¼‰
 * 2. æ¨¡æ‹Ÿæ¨¡å¼ï¼šå½“ native åº“ä¸å¯ç”¨æˆ–æ¨¡å‹æœªåŠ è½½æ—¶ï¼Œæä¾›åŸºç¡€å¯¹è¯èƒ½åŠ›
 */
class LlamaCpp {
    
    companion object {
        private const val TAG = "LlamaCpp"
        
        // Native åº“åŠ è½½çŠ¶æ€ï¼ˆå…¨å±€å•ä¾‹ï¼‰
        private val nativeLibLoaded = AtomicBoolean(false)
        private val nativeLoadError = AtomicReference<String?>(null)
        
        init {
            loadNativeLibrary()
        }
        
        /**
         * åŠ è½½ native åº“ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
         */
        private fun loadNativeLibrary() {
            try {
                System.loadLibrary("llama_android")
                nativeLibLoaded.set(true)
                nativeLoadError.set(null)
                Log.i(TAG, "âœ“ Native library 'llama_android' loaded successfully")
            } catch (e: UnsatisfiedLinkError) {
                nativeLibLoaded.set(false)
                nativeLoadError.set(e.message)
                Log.w(TAG, "âœ— Native library not available: ${e.message}")
                Log.i(TAG, "Falling back to Kotlin simulation mode")
            } catch (e: Exception) {
                nativeLibLoaded.set(false)
                nativeLoadError.set(e.message)
                Log.e(TAG, "âœ— Unexpected error loading native library: ${e.message}")
            }
        }
        
        /**
         * æ£€æŸ¥ native åº“æ˜¯å¦å·²åŠ è½½
         */
        fun isNativeAvailable(): Boolean = nativeLibLoaded.get()
        
        /**
         * è·å– native åº“åŠ è½½é”™è¯¯ä¿¡æ¯
         */
        fun getNativeLoadError(): String? = nativeLoadError.get()
    }
    
    // ==================== Native æ–¹æ³•å£°æ˜ ====================
    // è¿™äº›æ–¹æ³•åœ¨ llama_android.cpp ä¸­å®ç°ï¼Œä½¿ç”¨ JNI ç»‘å®š
    // æ–¹æ³•ç­¾åå¿…é¡»ä¸ C++ ç«¯å®Œå…¨åŒ¹é…
    
    private external fun nativeIsRealInferenceSupported(): Boolean
    private external fun nativeGetSystemInfo(): String
    private external fun nativeLoadModel(modelPath: String, nCtx: Int, nGpuLayers: Int): Boolean
    private external fun nativeUnloadModel()
    private external fun nativeIsModelLoaded(): Boolean
    private external fun nativeGenerate(prompt: String, maxTokens: Int, temperature: Float, callback: GenerationCallback)
    private external fun nativeStopGeneration()
    private external fun nativeIsGenerating(): Boolean
    
    // ==================== æ¨¡æ‹Ÿæ¨¡å¼çŠ¶æ€ ====================
    private val simulatedModelLoaded = AtomicBoolean(false)
    private val isCurrentlyGenerating = AtomicBoolean(false)
    private val shouldStop = AtomicBoolean(false)
    
    /**
     * å›è°ƒæ¥å£ - ç”¨äºæ¥æ”¶ç”Ÿæˆè¿‡ç¨‹ä¸­çš„äº‹ä»¶
     * Native å±‚å’Œ Kotlin å±‚éƒ½ä¼šè°ƒç”¨è¿™äº›æ–¹æ³•
     */
    interface GenerationCallback {
        /** æ”¶åˆ°ä¸€ä¸ª tokenï¼ˆæµå¼è¾“å‡ºï¼‰ */
        fun onToken(token: String)
        /** ç”Ÿæˆå®Œæˆ */
        fun onComplete(response: String)
        /** å‘ç”Ÿé”™è¯¯ */
        fun onError(error: String)
    }
    
    /**
     * æ£€æŸ¥æ˜¯å¦æ”¯æŒçœŸæ­£çš„æ¨ç†ï¼ˆllama.cpp æ˜¯å¦å·²ç¼–è¯‘å¹¶å¯ç”¨ï¼‰
     */
    fun isRealInferenceSupported(): Boolean {
        if (!nativeLibLoaded.get()) {
            return false
        }
        return try {
            nativeIsRealInferenceSupported()
        } catch (e: Exception) {
            Log.e(TAG, "Error checking real inference support: ${e.message}")
            false
        }
    }
    
    /**
     * è·å–ç³»ç»Ÿä¿¡æ¯ï¼ˆç”¨äºè¯Šæ–­ï¼‰
     */
    fun getSystemInfo(): String {
        if (!nativeLibLoaded.get()) {
            return buildSimulationModeInfo()
        }
        
        return try {
            val nativeInfo = nativeGetSystemInfo()
            buildString {
                appendLine("âœ“ Native åº“å·²åŠ è½½")
                appendLine()
                append(nativeInfo)
            }
        } catch (e: Exception) {
            Log.e(TAG, "Error getting system info: ${e.message}")
            "llama.cpp (native) - Error: ${e.message}"
        }
    }
    
    /**
     * æ„å»ºæ¨¡æ‹Ÿæ¨¡å¼ä¿¡æ¯
     */
    private fun buildSimulationModeInfo(): String {
        return buildString {
            appendLine("llama.cpp: Kotlin æ¨¡æ‹Ÿæ¨¡å¼")
            appendLine()
            appendLine("å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿ AIï¼Œå›å¤èƒ½åŠ›æœ‰é™ã€‚")
            appendLine()
            appendLine("Native åº“åŠ è½½çŠ¶æ€: ${if (nativeLibLoaded.get()) "å·²åŠ è½½" else "æœªåŠ è½½"}")
            nativeLoadError.get()?.let {
                appendLine("åŠ è½½é”™è¯¯: $it")
            }
            appendLine()
            appendLine("è¦å¯ç”¨çœŸæ­£çš„ç¦»çº¿ AI æ¨ç†ï¼Œéœ€è¦ï¼š")
            appendLine("1. è¿è¡Œ setup_llama.bat ä¸‹è½½ llama.cpp")
            appendLine("2. å°†é¡¹ç›®ç§»åŠ¨åˆ°çº¯è‹±æ–‡è·¯å¾„")
            appendLine("3. é‡æ–°æ„å»ºé¡¹ç›®")
            appendLine("4. ä¸‹è½½å¹¶åŠ è½½ GGUF æ¨¡å‹æ–‡ä»¶")
        }
    }
    
    /**
     * åŠ è½½æ¨¡å‹
     * 
     * @param modelPath æ¨¡å‹æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
     * @param nCtx ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤ 2048ï¼‰
     * @param nGpuLayers GPU åŠ é€Ÿå±‚æ•°ï¼ˆAndroid é»˜è®¤ 0ï¼‰
     * @return æ˜¯å¦åŠ è½½æˆåŠŸ
     */
    fun loadModel(modelPath: String, nCtx: Int = 2048, nGpuLayers: Int = 0): Boolean {
        Log.i(TAG, "Loading model: $modelPath (nCtx=$nCtx, nGpuLayers=$nGpuLayers)")
        
        if (!nativeLibLoaded.get()) {
            // æ¨¡æ‹Ÿæ¨¡å¼ï¼šæ ‡è®°ä¸ºå·²åŠ è½½
            Log.i(TAG, "Native not available, using simulation mode")
            simulatedModelLoaded.set(true)
            return true
        }
        
        return try {
            val result = nativeLoadModel(modelPath, nCtx, nGpuLayers)
            if (result) {
                simulatedModelLoaded.set(true)
                Log.i(TAG, "âœ“ Model loaded successfully via native")
            } else {
                Log.e(TAG, "âœ— Native loadModel returned false")
            }
            result
        } catch (e: Exception) {
            Log.e(TAG, "âœ— Exception during model loading: ${e.message}")
            // å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
            simulatedModelLoaded.set(true)
            Log.i(TAG, "Falling back to simulation mode after load failure")
            true
        }
    }
    
    /**
     * å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜
     */
    fun unloadModel() {
        Log.i(TAG, "Unloading model...")
        
        if (nativeLibLoaded.get()) {
            try {
                nativeUnloadModel()
                Log.i(TAG, "âœ“ Model unloaded via native")
            } catch (e: Exception) {
                Log.e(TAG, "Error unloading model: ${e.message}")
            }
        }
        
        simulatedModelLoaded.set(false)
    }
    
    /**
     * æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½ï¼ˆå¯ä»¥è¿›è¡Œæ¨ç†ï¼‰
     */
    fun isModelLoaded(): Boolean {
        if (!nativeLibLoaded.get()) {
            return simulatedModelLoaded.get()
        }
        
        return try {
            nativeIsModelLoaded()
        } catch (e: Exception) {
            Log.e(TAG, "Error checking model loaded state: ${e.message}")
            simulatedModelLoaded.get()
        }
    }
    
    /**
     * ç”Ÿæˆå›å¤ï¼ˆå¼‚æ­¥æ“ä½œï¼‰
     * 
     * è‡ªåŠ¨é€‰æ‹©æ¨ç†æ¨¡å¼ï¼š
     * - å¦‚æœ Native å¯ç”¨ä¸”æ¨¡å‹å·²åŠ è½½ â†’ ä½¿ç”¨ Native æ¨ç†
     * - å¦åˆ™ â†’ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
     * 
     * @param prompt è¾“å…¥æç¤ºè¯ï¼ˆé€šå¸¸æ˜¯ ChatML æ ¼å¼ï¼‰
     * @param maxTokens æœ€å¤§ç”Ÿæˆ token æ•°
     * @param temperature é‡‡æ ·æ¸©åº¦ï¼ˆ0-2ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
     * @param callback å›è°ƒæ¥å£
     */
    fun generate(
        prompt: String,
        maxTokens: Int = 256,
        temperature: Float = 0.7f,
        callback: GenerationCallback
    ) {
        // å†³å®šä½¿ç”¨å“ªç§æ¨¡å¼
        val nativeAvailable = nativeLibLoaded.get()
        val realInferenceSupported = nativeAvailable && isRealInferenceSupported()
        val modelLoaded = isModelLoaded()
        
        val useNative = nativeAvailable && realInferenceSupported && modelLoaded
        
        Log.i(TAG, buildString {
            append("Generate request - ")
            append("Native: $nativeAvailable, ")
            append("RealInference: $realInferenceSupported, ")
            append("ModelLoaded: $modelLoaded, ")
            append("UseNative: $useNative")
        })
        
        if (useNative) {
            try {
                Log.i(TAG, "â†’ Using native llama.cpp inference")
                nativeGenerate(prompt, maxTokens, temperature, callback)
                return
            } catch (e: Exception) {
                Log.w(TAG, "Native generate failed: ${e.message}, falling back to simulation")
            }
        }
        
        // ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
        Log.i(TAG, "â†’ Using Kotlin simulation mode")
        generateSimulated(prompt, callback)
    }
    
    /**
     * æ¨¡æ‹Ÿæ¨¡å¼ç”Ÿæˆï¼ˆåœ¨åå°çº¿ç¨‹æ‰§è¡Œï¼‰
     */
    private fun generateSimulated(prompt: String, callback: GenerationCallback) {
        // é˜²æ­¢é‡å…¥
        if (!isCurrentlyGenerating.compareAndSet(false, true)) {
            callback.onError("å·²æœ‰ç”Ÿæˆä»»åŠ¡åœ¨è¿›è¡Œä¸­")
            return
        }
        
        shouldStop.set(false)
        
        Thread {
            try {
                val response = generateSimulatedResponse(prompt)
                streamOutput(response, callback)
            } catch (e: Exception) {
                Log.e(TAG, "Simulation error: ${e.message}")
                callback.onError(e.message ?: "æœªçŸ¥é”™è¯¯")
            } finally {
                isCurrentlyGenerating.set(false)
            }
        }.apply {
            name = "LlamaCpp-SimulationThread"
            start()
        }
    }
    
    /**
     * æ¨¡æ‹Ÿæµå¼è¾“å‡º
     */
    private fun streamOutput(response: String, callback: GenerationCallback) {
        val fullResponse = StringBuilder()
        var pos = 0
        
        while (pos < response.length && !shouldStop.get()) {
            // æ¯æ¬¡è¾“å‡º 2-4 ä¸ªå­—ç¬¦ï¼Œæ¨¡æ‹ŸçœŸå®æ¨ç†
            val chunkSize = (2..4).random()
            val end = minOf(pos + chunkSize, response.length)
            val token = response.substring(pos, end)
            
            fullResponse.append(token)
            callback.onToken(token)
            pos = end
            
            // æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
            Thread.sleep((20L..40L).random())
        }
        
        callback.onComplete(fullResponse.toString())
    }
    
    /**
     * ç”Ÿæˆæ¨¡æ‹Ÿå›å¤
     * æ ¹æ®ç”¨æˆ·è¾“å…¥è¿”å›é¢„è®¾çš„å›å¤
     */
    private fun generateSimulatedResponse(prompt: String): String {
        // ä» ChatML æ ¼å¼ä¸­æå–ç”¨æˆ·åŸå§‹æ¶ˆæ¯
        val userMessage = extractUserMessage(prompt)
        val lowerPrompt = userMessage.lowercase()
        
        return when {
            // é—®å€™è¯­
            containsAny(lowerPrompt, "ä½ å¥½", "hello", "hi", "å—¨", "æ‚¨å¥½") ->
                buildGreetingResponse()
            
            // è‡ªæˆ‘ä»‹ç»
            containsAny(lowerPrompt, "ä½ æ˜¯è°", "ä»‹ç»", "ä»€ä¹ˆæ˜¯ai", "ä½ å«ä»€ä¹ˆ") ->
                buildIntroductionResponse()
            
            // å¤©æ°”æŸ¥è¯¢
            lowerPrompt.contains("å¤©æ°”") ->
                buildWeatherResponse()
            
            // æ—¶é—´æŸ¥è¯¢
            containsAny(lowerPrompt, "æ—¶é—´", "å‡ ç‚¹", "ç°åœ¨") ->
                "ğŸ• å½“å‰æ—¶é—´æ˜¯ ${java.text.SimpleDateFormat("HH:mm:ss", java.util.Locale.getDefault()).format(java.util.Date())}"
            
            // æ—¥æœŸæŸ¥è¯¢
            containsAny(lowerPrompt, "æ—¥æœŸ", "ä»Šå¤©", "æ˜ŸæœŸ", "å‘¨å‡ ") ->
                "ğŸ“… ä»Šå¤©æ˜¯ ${java.text.SimpleDateFormat("yyyyå¹´MMæœˆddæ—¥ EEEE", java.util.Locale.CHINESE).format(java.util.Date())}"
            
            // å¸®åŠ©/åŠŸèƒ½
            containsAny(lowerPrompt, "å¸®åŠ©", "åŠŸèƒ½", "èƒ½åšä»€ä¹ˆ", "ä½ ä¼š") ->
                buildHelpResponse()
            
            // è®¡ç®—ç±»é—®é¢˜
            containsAny(lowerPrompt, "+", "-", "*", "/", "ç­‰äº", "è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤") ->
                tryCalculate(userMessage)
            
            // ç¼–ç¨‹ç›¸å…³
            containsAny(lowerPrompt, "ä»£ç ", "code", "ç¼–ç¨‹", "java", "kotlin", "python", "ç¨‹åº") ->
                buildProgrammingResponse()
            
            // æ„Ÿè°¢
            containsAny(lowerPrompt, "è°¢è°¢", "æ„Ÿè°¢", "thank") ->
                "ä¸å®¢æ°”ï¼ğŸ˜Š å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ã€‚æœ‰ä»»ä½•é—®é¢˜éšæ—¶é—®æˆ‘ï¼"
            
            // é»˜è®¤å›å¤
            else -> buildDefaultResponse(userMessage)
        }
    }
    
    /**
     * è¾…åŠ©æ–¹æ³•ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯
     */
    private fun containsAny(text: String, vararg keywords: String): Boolean {
        return keywords.any { text.contains(it) }
    }
    
    /**
     * æ„å»ºé—®å€™å›å¤
     */
    private fun buildGreetingResponse(): String {
        return buildString {
            appendLine("ä½ å¥½ï¼æˆ‘æ˜¯è¿è¡Œåœ¨ä½ è®¾å¤‡ä¸Šçš„æœ¬åœ° AI åŠ©æ‰‹ã€‚")
            appendLine()
            appendLine("ğŸ§  å½“å‰æ¨¡å¼ï¼šæœ¬åœ°æ¨¡æ‹Ÿ AI")
            appendLine("ğŸ“± ä¸éœ€è¦ç½‘ç»œè¿æ¥")
            appendLine("ğŸ”’ éšç§æ•°æ®ä¸ä¸Šä¼ ")
            appendLine()
            append("å¦‚éœ€æ›´å¼ºå¤§çš„ AI åŠŸèƒ½ï¼Œè¯·å¼€å¯è”ç½‘æ¨¡å¼ã€‚")
        }
    }
    
    /**
     * æ„å»ºè‡ªæˆ‘ä»‹ç»å›å¤
     */
    private fun buildIntroductionResponse(): String {
        return buildString {
            appendLine("æˆ‘æ˜¯ DeepSeek AI åŠ©æ‰‹ï¼Œè¿è¡Œåœ¨ä½ çš„ Android è®¾å¤‡ä¸Šã€‚")
            appendLine()
            appendLine("ğŸ“± **æœ¬åœ° AI æ¨¡å¼**ï¼ˆå½“å‰ï¼‰ï¼š")
            appendLine("â€¢ ä¸éœ€è¦ç½‘ç»œè¿æ¥")
            appendLine("â€¢ åŸºç¡€å¯¹è¯èƒ½åŠ›")
            appendLine("â€¢ éšç§æ•°æ®ä¸ä¸Šä¼ ")
            appendLine()
            appendLine("ğŸŒ **è”ç½‘æ¨¡å¼**ï¼š")
            appendLine("â€¢ è°ƒç”¨äº‘ç«¯ AI æ¥å£")
            appendLine("â€¢ æ›´å¼ºå¤§çš„ç†è§£èƒ½åŠ›")
            append("â€¢ éœ€è¦ç½‘ç»œå’Œ API Key")
        }
    }
    
    /**
     * æ„å»ºå¤©æ°”æŸ¥è¯¢å›å¤
     */
    private fun buildWeatherResponse(): String {
        return buildString {
            appendLine("æŠ±æ­‰ï¼Œæˆ‘æ˜¯ç¦»çº¿è¿è¡Œçš„æœ¬åœ° AIï¼Œæ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚")
            appendLine()
            appendLine("ğŸŒ¤ï¸ å»ºè®®ä½ ï¼š")
            appendLine("â€¢ å¼€å¯è”ç½‘æ¨¡å¼æŸ¥è¯¢å¤©æ°”")
            append("â€¢ æˆ–ä½¿ç”¨å¤©æ°”åº”ç”¨æŸ¥è¯¢")
        }
    }
    
    /**
     * æ„å»ºå¸®åŠ©å›å¤
     */
    private fun buildHelpResponse(): String {
        return buildString {
            appendLine("æˆ‘æ˜¯æœ¬åœ° AI åŠ©æ‰‹ï¼Œå¯ä»¥å¸®ä½ ï¼š")
            appendLine()
            appendLine("ğŸ’¬ **åŸºç¡€å¯¹è¯**")
            appendLine("â€¢ å›ç­”ç®€å•é—®é¢˜")
            appendLine("â€¢ æ—¥å¸¸èŠå¤©äº¤æµ")
            appendLine()
            appendLine("ğŸ• **æ—¶é—´æ—¥æœŸ**")
            appendLine("â€¢ æŸ¥è¯¢å½“å‰æ—¶é—´")
            appendLine("â€¢ æŸ¥è¯¢ä»Šå¤©æ—¥æœŸ")
            appendLine()
            appendLine("ğŸ§® **ç®€å•è®¡ç®—**")
            appendLine("â€¢ åŸºç¡€å››åˆ™è¿ç®—")
            appendLine()
            appendLine("ğŸ’¡ **å°æç¤º**")
            append("å¼€å¯è”ç½‘æ¨¡å¼å¯è·å¾—æ›´å¼ºå¤§çš„ AI èƒ½åŠ›ï¼")
        }
    }
    
    /**
     * æ„å»ºç¼–ç¨‹ç›¸å…³å›å¤
     */
    private fun buildProgrammingResponse(): String {
        return buildString {
            appendLine("ğŸ’» ç¼–ç¨‹é—®é¢˜éœ€è¦æ›´å¼ºå¤§çš„ AI èƒ½åŠ›ã€‚")
            appendLine()
            appendLine("å»ºè®®å¼€å¯è”ç½‘æ¨¡å¼ï¼Œè¿æ¥äº‘ç«¯ AI æ¥è·å–ï¼š")
            appendLine("â€¢ ä»£ç ç”Ÿæˆ")
            appendLine("â€¢ ä»£ç è§£é‡Š")
            append("â€¢ Bug è°ƒè¯•å»ºè®®")
        }
    }
    
    /**
     * æ„å»ºé»˜è®¤å›å¤
     */
    private fun buildDefaultResponse(userMessage: String): String {
        val shortQuestion = if (userMessage.length > 30) {
            userMessage.take(30) + "..."
        } else {
            userMessage
        }
        
        return buildString {
            appendLine("æˆ‘æ”¶åˆ°äº†ä½ çš„æ¶ˆæ¯ï¼šã€Œ$shortQuestionã€")
            appendLine()
            appendLine("ğŸ¤” å½“å‰æˆ‘å¤„äºæœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ï¼Œå›å¤èƒ½åŠ›æœ‰é™ã€‚")
            appendLine()
            appendLine("ä½ å¯ä»¥ï¼š")
            appendLine("â€¢ è¯¢é—®å½“å‰æ—¶é—´/æ—¥æœŸ")
            appendLine("â€¢ è¿›è¡Œç®€å•å¯¹è¯")
            append("â€¢ å¼€å¯è”ç½‘æ¨¡å¼è·å¾—å®Œæ•´ AI èƒ½åŠ›")
        }
    }
    
    /**
     * ä» ChatML æ ¼å¼çš„ prompt ä¸­æå–ç”¨æˆ·åŸå§‹æ¶ˆæ¯
     */
    private fun extractUserMessage(prompt: String): String {
        // å°è¯•ä» ChatML æ ¼å¼æå–
        val userStartTag = "<|im_start|>user"
        val userEndTag = "<|im_end|>"
        
        val startIndex = prompt.indexOf(userStartTag)
        if (startIndex != -1) {
            val messageStart = startIndex + userStartTag.length
            val endIndex = prompt.indexOf(userEndTag, messageStart)
            if (endIndex != -1) {
                return prompt.substring(messageStart, endIndex).trim()
            }
        }
        
        // å¦‚æœä¸æ˜¯ ChatML æ ¼å¼ï¼Œç›´æ¥è¿”å›åŸå§‹ prompt
        return prompt.trim()
    }
    
    /**
     * å°è¯•è®¡ç®—ç®€å•çš„æ•°å­¦è¡¨è¾¾å¼
     */
    private fun tryCalculate(expression: String): String {
        return try {
            // é¢„å¤„ç†è¡¨è¾¾å¼
            val cleaned = expression.replace(" ", "")
                .replace("åŠ ", "+")
                .replace("å‡", "-")
                .replace("ä¹˜", "*")
                .replace("ä¹˜ä»¥", "*")
                .replace("é™¤", "/")
                .replace("é™¤ä»¥", "/")
                .replace("ç­‰äº", "=")
                .replace("Ã—", "*")
                .replace("Ã·", "/")
            
            // ç®€å•çš„ä¸¤ä¸ªæ•°è¿ç®—
            val pattern = Regex("""(\d+\.?\d*)\s*([+\-*/])\s*(\d+\.?\d*)""")
            val match = pattern.find(cleaned)
            
            if (match != null) {
                val (num1Str, op, num2Str) = match.destructured
                val num1 = num1Str.toDouble()
                val num2 = num2Str.toDouble()
                
                val result = when (op) {
                    "+" -> num1 + num2
                    "-" -> num1 - num2
                    "*" -> num1 * num2
                    "/" -> if (num2 != 0.0) num1 / num2 else Double.NaN
                    else -> Double.NaN
                }
                
                if (result.isNaN()) {
                    "æ— æ³•è®¡ç®—ï¼ˆé™¤æ•°ä¸èƒ½ä¸º0ï¼‰"
                } else {
                    val formatted = if (result == result.toLong().toDouble()) {
                        result.toLong().toString()
                    } else {
                        String.format("%.2f", result)
                    }
                    "ğŸ§® è®¡ç®—ç»“æœï¼š$num1Str $op $num2Str = $formatted"
                }
            } else {
                "è¯·è¾“å…¥ç®€å•çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š3+5 æˆ– 10*2"
            }
        } catch (e: Exception) {
            "æŠ±æ­‰ï¼Œæ— æ³•è®¡ç®—è¿™ä¸ªè¡¨è¾¾å¼ã€‚è¯·å°è¯•ç®€å•çš„æ ¼å¼ï¼Œå¦‚ï¼š5+3"
        }
    }
    
    /**
     * åœæ­¢å½“å‰ç”Ÿæˆä»»åŠ¡
     */
    fun stopGeneration() {
        Log.i(TAG, "Stop generation requested")
        shouldStop.set(true)
        
        if (nativeLibLoaded.get()) {
            try {
                nativeStopGeneration()
            } catch (e: Exception) {
                Log.e(TAG, "Error stopping native generation: ${e.message}")
            }
        }
    }
    
    /**
     * æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç”Ÿæˆ
     */
    fun isGenerating(): Boolean {
        if (!nativeLibLoaded.get()) {
            return isCurrentlyGenerating.get()
        }
        
        return try {
            nativeIsGenerating()
        } catch (e: Exception) {
            isCurrentlyGenerating.get()
        }
    }
    
    /**
     * è·å–å½“å‰æ¨¡å¼æè¿°ï¼ˆç”¨äº UI æ˜¾ç¤ºï¼‰
     */
    fun getModeDescription(): String {
        return if (nativeLibLoaded.get() && isRealInferenceSupported()) {
            if (isModelLoaded()) {
                "ğŸ§  çœŸå® AI æ¨ç† (llama.cpp native)"
            } else {
                "ğŸ§  Native å°±ç»ª (æ¨¡å‹æœªåŠ è½½)"
            }
        } else if (nativeLibLoaded.get()) {
            "âš™ï¸ Native åº“å·²åŠ è½½ (llama.cpp æœªç¼–è¯‘)"
        } else {
            "ğŸ’¡ æ¨¡æ‹Ÿ AI æ¨¡å¼ (å›å¤èƒ½åŠ›æœ‰é™)"
        }
    }
    
    /**
     * è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
     */
    fun getDetailedStatus(): Map<String, Any> {
        return mapOf(
            "nativeLoaded" to nativeLibLoaded.get(),
            "nativeLoadError" to (nativeLoadError.get() ?: "none"),
            "realInferenceSupported" to isRealInferenceSupported(),
            "modelLoaded" to isModelLoaded(),
            "isGenerating" to isGenerating(),
            "mode" to getModeDescription()
        )
    }
}

/**
 * ç®€å•çš„ç”Ÿæˆå›è°ƒå®ç°
 * æ–¹ä¾¿åˆ›å»ºå›è°ƒå®ä¾‹
 */
open class SimpleGenerationCallback(
    private val onTokenReceived: (String) -> Unit = {},
    private val onGenerationComplete: (String) -> Unit = {},
    private val onGenerationError: (String) -> Unit = {}
) : LlamaCpp.GenerationCallback {
    
    override fun onToken(token: String) {
        onTokenReceived(token)
    }
    
    override fun onComplete(response: String) {
        onGenerationComplete(response)
    }
    
    override fun onError(error: String) {
        onGenerationError(error)
    }
}
